{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90b9fbac-4c86-406f-b6dc-c4f4d6f1ae40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "\n",
    "# Clear GPU memory (if applicable)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63a0c75a-2579-439a-bfc5-bb66f4599a4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.13s/it]\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.1-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 498/498 [00:00<00:00, 1633.00 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:00<00:00, 1430.36 examples/s]\n",
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1876117/1192820909.py:159: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/tmp/ipykernel_1876117/1192820909.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmghasemizade97\u001b[0m (\u001b[33mmghasemizade97-university-of-vermont\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/u/mghasemizade/wandb/run-20241212_183231-nv8aehyf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mghasemizade97-university-of-vermont/huggingface/runs/nv8aehyf' target=\"_blank\">sequence_classification</a></strong> to <a href='https://wandb.ai/mghasemizade97-university-of-vermont/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mghasemizade97-university-of-vermont/huggingface' target=\"_blank\">https://wandb.ai/mghasemizade97-university-of-vermont/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mghasemizade97-university-of-vermont/huggingface/runs/nv8aehyf' target=\"_blank\">https://wandb.ai/mghasemizade97-university-of-vermont/huggingface/runs/nv8aehyf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 207\u001b[0m\n\u001b[1;32m    184\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m    185\u001b[0m     output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequence_classification\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    186\u001b[0m     learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-4\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    193\u001b[0m     load_best_model_at_end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    194\u001b[0m )\n\u001b[1;32m    196\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomTrainer(\n\u001b[1;32m    197\u001b[0m     model \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m    198\u001b[0m     args \u001b[38;5;241m=\u001b[39m training_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    204\u001b[0m     class_weights\u001b[38;5;241m=\u001b[39mclass_weights,\n\u001b[1;32m    205\u001b[0m )\n\u001b[0;32m--> 207\u001b[0m train_result \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_predictions\u001b[39m(model, df):\n\u001b[1;32m    212\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39minput\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   2124\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   2125\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   2126\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   2127\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   2128\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/site-packages/transformers/trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2479\u001b[0m )\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/site-packages/transformers/trainer.py:3579\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3578\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3579\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs, num_items_in_batch\u001b[38;5;241m=\u001b[39mnum_items_in_batch)\n\u001b[1;32m   3581\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3584\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3585\u001b[0m ):\n",
      "Cell \u001b[0;32mIn[1], line 171\u001b[0m, in \u001b[0;36mCustomTrainer.compute_loss\u001b[0;34m(self, model, inputs, num_items_in_batch, return_outputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Extract logits assuming they are directly outputted by the model\u001b[39;00m\n\u001b[1;32m    174\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:193\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    192\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[: \u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 193\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, module_kwargs)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:212\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any\n\u001b[1;32m    211\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parallel_apply(\n\u001b[1;32m    213\u001b[0m         replicas, inputs, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[: \u001b[38;5;28mlen\u001b[39m(replicas)]\n\u001b[1;32m    214\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:118\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    116\u001b[0m         thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m thread \u001b[38;5;129;01min\u001b[39;00m threads:\n\u001b[0;32m--> 118\u001b[0m         thread\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     _worker(\u001b[38;5;241m0\u001b[39m, modules[\u001b[38;5;241m0\u001b[39m], inputs[\u001b[38;5;241m0\u001b[39m], kwargs_tup[\u001b[38;5;241m0\u001b[39m], devices[\u001b[38;5;241m0\u001b[39m], streams[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/threading.py:1119\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1119\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock()\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/threading.py:1139\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lock\u001b[38;5;241m.\u001b[39macquire(block, timeout):\n\u001b[1;32m   1140\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import functools\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import evaluate\n",
    "import re\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, balanced_accuracy_score, accuracy_score\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from datasets import Dataset, DatasetDict\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "df = pd.read_csv('filtered_labeled.csv')\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'(\\n[a-zA-Z])', '', text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    return cleaned_text\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "df['Institutional_Form'] = df['Institutional_Form'].astype('category')\n",
    "df['Institutional_Form_category'] = df['Institutional_Form'].cat.codes\n",
    "\n",
    "category_map = {code: category for code, category in enumerate(df['Institutional_Form'].cat.categories)}\n",
    "\n",
    "df_train, df_val = train_test_split(df, train_size=0.8, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "def generate_features_with_prompt(df):\n",
    "    # Define the instruction\n",
    "    instruction = (\n",
    "        \"You are a contract classification assistant. Your task is to classify the contract text \"\n",
    "        \"into one of the predefined categories. Here are the criteria for each category:\\n\"\n",
    "        \"- Joint Operations: Partnership arrangements to jointly produce services with one or more organizations.\\n\"\n",
    "        \"- New Joint Entities: Two or more organizations creating a separate new entity to manage or govern a shared asset or service.\\n\"\n",
    "        \"- Resource Sharing: Sharing of information, personnel, equipment, etc., between governments or community organizations to provide services.\\n\"\n",
    "        \"- Service Contracts: Agreements with outside entities, public or private, for provision or support services.\\n\"\n",
    "        \"Analyze the given text carefully and respond with the appropriate category.\"\n",
    "    )\n",
    "    df['Institutional_Form'] = df['Institutional_Form'].astype(str)\n",
    "    # Generate the `input` column by combining instruction, text, and category\n",
    "    df['input'] = (\n",
    "        instruction\n",
    "        + \"\\n\\nContract Text: \"\n",
    "        + df['text']\n",
    "        + \"\\n\\nInstitutional Form Category: \"\n",
    "        + df['Institutional_Form']\n",
    "    )\n",
    "\n",
    "    # Add the numerical category column if not already present\n",
    "    if 'Institutional_Form_category' not in df.columns:\n",
    "        df['Institutional_Form'] = df['Institutional_Form'].astype('category')\n",
    "        df['Institutional_Form_category'] = df['Institutional_Form'].cat.codes\n",
    "\n",
    "    return df\n",
    "\n",
    "generate_features_with_prompt(df_train)\n",
    "generate_features_with_prompt(df_val)\n",
    "\n",
    "dataset_train = Dataset.from_pandas(df_train.reset_index(drop=True))\n",
    "dataset_val = Dataset.from_pandas(df_val.reset_index(drop=True))\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': dataset_train,\n",
    "    'val': dataset_val,\n",
    "})\n",
    "\n",
    "#Since our classes are not balanced let's calculate class weights based on inverse value counts\n",
    "#Convert to pytorch tensor since we will need it\n",
    "df_train.Institutional_Form_category.value_counts(normalize=True)\n",
    "class_weights=(1/df_train.Institutional_Form.value_counts(normalize=True).sort_index()).tolist()\n",
    "class_weights=torch.tensor(class_weights)\n",
    "class_weights=class_weights/class_weights.sum()\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "#Quantization Config (for QLORA)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True, # enable 4-bit quantization\n",
    "    bnb_4bit_quant_type = 'nf4', # information theoretically optimal dtype for normally distributed weights\n",
    "    bnb_4bit_use_double_quant = True, # quantize quantized weights //insert xzibit meme\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16 # optimized fp format for ML\n",
    ")\n",
    "\n",
    "#\n",
    "lora_config = LoraConfig(\n",
    "    r = 16, # the dimension of the low-rank matrices\n",
    "    lora_alpha = 8, # scaling factor for LoRA activations vs pre-trained weight activations\n",
    "    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "    lora_dropout = 0.05, # dropout probability of the LoRA layers\n",
    "    bias = 'none', # wether to train bias weights, set to 'none' for attention layers\n",
    "    task_type = 'SEQ_CLS'\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    num_labels=len(category_map)\n",
    ")\n",
    "\n",
    "#prepare_model_for_kbit_training() function to preprocess the quantized model for training.\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "#Must use .cache = False as below or it crashes from my experience\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "def llama_preprocessing_function(examples):\n",
    "    return tokenizer(examples['input'], truncation=True, max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(llama_preprocessing_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"Institutional_Form_category\", \"label\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "#Purpose: Automatically pads text data to the longest sequence in a batch\n",
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    try:\n",
    "        # it's a classification task, take the argmax\n",
    "        predictions_processed = np.argmax(predictions, axis=1)\n",
    "\n",
    "        # Calculate Pearson correlation\n",
    "        pearson, _ = pearsonr(predictions_processed, labels)\n",
    "\n",
    "        return {'pearson': pearson}\n",
    "    except Exception as e:\n",
    "        print(f\"Error in compute_metrics: {e}\")\n",
    "        return {'pearson': None}\n",
    "    \n",
    "#We will have a custom loss function that deals with the class weights and have class weights as additional argument in constructor\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Ensure label_weights is a tensor\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, num_items_in_batch=None, return_outputs=False):\n",
    "        # Extract labels and convert them to long type for cross_entropy\n",
    "        labels = inputs.pop(\"labels\").long()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Extract logits assuming they are directly outputted by the model\n",
    "        logits = outputs.get('logits')\n",
    "\n",
    "        # Compute custom loss with class weights for imbalanced data handling\n",
    "        if self.class_weights is not None:\n",
    "            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "training_args = TrainingArguments(\n",
    "    output_dir = 'sequence_classification',\n",
    "    learning_rate = 1e-4,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    num_train_epochs = 3,\n",
    "    weight_decay = 0.01,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end = True\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_datasets['train'],\n",
    "    eval_dataset = tokenized_datasets['val'],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = collate_fn,\n",
    "    compute_metrics = compute_metrics,\n",
    "    class_weights=class_weights,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "def make_predictions(model, df):\n",
    "    \n",
    "\n",
    "    sentences = df.input.tolist()\n",
    "\n",
    "      # Define the batch size\n",
    "    batch_size = 32  # You can adjust this based on your system's memory capacity\n",
    "\n",
    "      # Initialize an empty list to store the model outputs\n",
    "    all_outputs = []\n",
    "\n",
    "      # Process the sentences in batches\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "          # Get the batch of sentences\n",
    "        batch_sentences = sentences[i:i + batch_size]\n",
    "\n",
    "          # Tokenize the batch\n",
    "        inputs = tokenizer(batch_sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "          # Move tensors to the device where the model is (e.g., GPU or CPU)\n",
    "        inputs = {k: v.to('cuda' if torch.cuda.is_available() else 'cpu') for k, v in inputs.items()}\n",
    "\n",
    "          # Perform inference and store the logits\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            all_outputs.append(outputs['logits'])\n",
    "\n",
    "    final_outputs = torch.cat(all_outputs, dim=0)\n",
    "    df['predictions']=final_outputs.argmax(axis=1).cpu().numpy()\n",
    "    #df['predictions']=df['predictions'].apply(lambda l:category_map[l])\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_performance_metrics(df):\n",
    "    y_test = df.Institutional_Form_category\n",
    "    y_pred = df.predictions\n",
    "    print(f\"comparing test {y_test} and pred {y_pred}\")\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(\"Balanced Accuracy Score:\", balanced_accuracy_score(y_test, y_pred))\n",
    "    print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "    \n",
    "df_val = make_predictions(model, df_val)\n",
    "get_performance_metrics(df_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c1efb85-b7f5-440c-ab1f-05d16b926fa8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparing test 249    0\n",
      "558    3\n",
      "174    1\n",
      "280    0\n",
      "110    1\n",
      "      ..\n",
      "6      3\n",
      "104    3\n",
      "114    3\n",
      "355    3\n",
      "132    3\n",
      "Name: Institutional_Form_category, Length: 125, dtype: int8 and pred 249    3\n",
      "558    3\n",
      "174    3\n",
      "280    3\n",
      "110    3\n",
      "      ..\n",
      "6      3\n",
      "104    1\n",
      "114    1\n",
      "355    1\n",
      "132    3\n",
      "Name: predictions, Length: 125, dtype: int64\n",
      "Confusion Matrix:\n",
      "[[ 3  5  6  7]\n",
      " [ 7  2  4  8]\n",
      " [ 2  1  2  5]\n",
      " [27 15 13 18]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.14      0.10        21\n",
      "           1       0.09      0.10      0.09        21\n",
      "           2       0.08      0.20      0.11        10\n",
      "           3       0.47      0.25      0.32        73\n",
      "\n",
      "    accuracy                           0.20       125\n",
      "   macro avg       0.18      0.17      0.16       125\n",
      "weighted avg       0.31      0.20      0.23       125\n",
      "\n",
      "Balanced Accuracy Score: 0.17116764514024788\n",
      "Accuracy Score: 0.2\n"
     ]
    }
   ],
   "source": [
    "def make_predictions(model, df):\n",
    "    \n",
    "\n",
    "    sentences = df.input.tolist()\n",
    "\n",
    "      # Define the batch size\n",
    "    batch_size = 32  # You can adjust this based on your system's memory capacity\n",
    "\n",
    "      # Initialize an empty list to store the model outputs\n",
    "    all_outputs = []\n",
    "\n",
    "      # Process the sentences in batches\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "          # Get the batch of sentences\n",
    "        batch_sentences = sentences[i:i + batch_size]\n",
    "\n",
    "          # Tokenize the batch\n",
    "        inputs = tokenizer(batch_sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "          # Move tensors to the device where the model is (e.g., GPU or CPU)\n",
    "        inputs = {k: v.to('cuda' if torch.cuda.is_available() else 'cpu') for k, v in inputs.items()}\n",
    "\n",
    "          # Perform inference and store the logits\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            all_outputs.append(outputs['logits'])\n",
    "\n",
    "    final_outputs = torch.cat(all_outputs, dim=0)\n",
    "    df['predictions']=final_outputs.argmax(axis=1).cpu().numpy()\n",
    "    #df['predictions']=df['predictions'].apply(lambda l:category_map[l])\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_performance_metrics(df):\n",
    "    y_test = df.Institutional_Form_category\n",
    "    y_pred = df.predictions\n",
    "    print(f\"comparing test {y_test} and pred {y_pred}\")\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(\"Balanced Accuracy Score:\", balanced_accuracy_score(y_test, y_pred))\n",
    "    print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "    \n",
    "df_val = make_predictions(model, df_val)\n",
    "get_performance_metrics(df_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48decb9a-03d6-49f2-9041-b7999917bb40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.13s/it]\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.1-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 498/498 [00:00<00:00, 1666.63 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:00<00:00, 1424.16 examples/s]\n",
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1897378/486941468.py:159: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with learning_rate=1e-05, weight_decay=0.001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1897378/486941468.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmghasemizade97\u001b[0m (\u001b[33mmghasemizade97-university-of-vermont\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/u/mghasemizade/wandb/run-20241212_183304-5s0totvw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mghasemizade97-university-of-vermont/huggingface/runs/5s0totvw' target=\"_blank\">sequence_classification_lr1e-05_wd0.001</a></strong> to <a href='https://wandb.ai/mghasemizade97-university-of-vermont/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mghasemizade97-university-of-vermont/huggingface' target=\"_blank\">https://wandb.ai/mghasemizade97-university-of-vermont/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mghasemizade97-university-of-vermont/huggingface/runs/5s0totvw' target=\"_blank\">https://wandb.ai/mghasemizade97-university-of-vermont/huggingface/runs/5s0totvw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/24 34:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.066216</td>\n",
       "      <td>-0.031263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.927700</td>\n",
       "      <td>2.985620</td>\n",
       "      <td>-0.041582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.740300</td>\n",
       "      <td>2.957715</td>\n",
       "      <td>-0.012257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1897378/486941468.py:159: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparing test 249    0\n",
      "558    3\n",
      "174    1\n",
      "280    0\n",
      "110    1\n",
      "      ..\n",
      "6      3\n",
      "104    3\n",
      "114    3\n",
      "355    3\n",
      "132    3\n",
      "Name: Institutional_Form_category, Length: 125, dtype: int8 and pred 249    3\n",
      "558    1\n",
      "174    3\n",
      "280    3\n",
      "110    3\n",
      "      ..\n",
      "6      3\n",
      "104    0\n",
      "114    3\n",
      "355    1\n",
      "132    3\n",
      "Name: predictions, Length: 125, dtype: int64\n",
      "Confusion Matrix:\n",
      "[[ 7  7  1  6]\n",
      " [ 5  9  0  7]\n",
      " [ 4  3  0  3]\n",
      " [20 30  4 19]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.33      0.25        21\n",
      "           1       0.18      0.43      0.26        21\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.54      0.26      0.35        73\n",
      "\n",
      "    accuracy                           0.28       125\n",
      "   macro avg       0.23      0.26      0.21       125\n",
      "weighted avg       0.38      0.28      0.29       125\n",
      "\n",
      "Balanced Accuracy Score: 0.2555446836268754\n",
      "Accuracy Score: 0.28\n",
      "Results for learning_rate=1e-05, weight_decay=0.001\n",
      "Train Loss: 2.7909214893976846\n",
      "Evaluation Metrics: {'eval_loss': 2.957714557647705, 'eval_pearson': -0.012256786452980261, 'eval_runtime': 56.2775, 'eval_samples_per_second': 2.221, 'eval_steps_per_second': 0.036, 'epoch': 3.0}\n",
      "Performance Metrics: None\n",
      "\n",
      "Training with learning_rate=1e-05, weight_decay=0.01\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1897378/486941468.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n",
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/24 34:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.863046</td>\n",
       "      <td>-0.054032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.616800</td>\n",
       "      <td>2.785446</td>\n",
       "      <td>-0.019659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.491900</td>\n",
       "      <td>2.759545</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1897378/486941468.py:159: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparing test 249    0\n",
      "558    3\n",
      "174    1\n",
      "280    0\n",
      "110    1\n",
      "      ..\n",
      "6      3\n",
      "104    3\n",
      "114    3\n",
      "355    3\n",
      "132    3\n",
      "Name: Institutional_Form_category, Length: 125, dtype: int8 and pred 249    3\n",
      "558    1\n",
      "174    3\n",
      "280    3\n",
      "110    3\n",
      "      ..\n",
      "6      3\n",
      "104    0\n",
      "114    3\n",
      "355    1\n",
      "132    3\n",
      "Name: predictions, Length: 125, dtype: int64\n",
      "Confusion Matrix:\n",
      "[[ 7  8  1  5]\n",
      " [ 5  8  1  7]\n",
      " [ 4  3  1  2]\n",
      " [23 23 10 17]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.33      0.23        21\n",
      "           1       0.19      0.38      0.25        21\n",
      "           2       0.08      0.10      0.09        10\n",
      "           3       0.55      0.23      0.33        73\n",
      "\n",
      "    accuracy                           0.26       125\n",
      "   macro avg       0.25      0.26      0.23       125\n",
      "weighted avg       0.39      0.26      0.28       125\n",
      "\n",
      "Balanced Accuracy Score: 0.2617906066536203\n",
      "Accuracy Score: 0.264\n",
      "Results for learning_rate=1e-05, weight_decay=0.01\n",
      "Train Loss: 2.5295271078745523\n",
      "Evaluation Metrics: {'eval_loss': 2.759544849395752, 'eval_pearson': 0.0007004502010553668, 'eval_runtime': 56.6, 'eval_samples_per_second': 2.208, 'eval_steps_per_second': 0.035, 'epoch': 3.0}\n",
      "Performance Metrics: None\n",
      "\n",
      "Training with learning_rate=1e-05, weight_decay=0.1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1897378/486941468.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n",
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/24 34:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.697294</td>\n",
       "      <td>-0.057587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.382200</td>\n",
       "      <td>2.626969</td>\n",
       "      <td>0.017276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.308700</td>\n",
       "      <td>2.605075</td>\n",
       "      <td>0.039684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1897378/486941468.py:159: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparing test 249    0\n",
      "558    3\n",
      "174    1\n",
      "280    0\n",
      "110    1\n",
      "      ..\n",
      "6      3\n",
      "104    3\n",
      "114    3\n",
      "355    3\n",
      "132    3\n",
      "Name: Institutional_Form_category, Length: 125, dtype: int8 and pred 249    3\n",
      "558    1\n",
      "174    3\n",
      "280    3\n",
      "110    2\n",
      "      ..\n",
      "6      3\n",
      "104    0\n",
      "114    3\n",
      "355    1\n",
      "132    3\n",
      "Name: predictions, Length: 125, dtype: int64\n",
      "Confusion Matrix:\n",
      "[[ 7  8  1  5]\n",
      " [ 5  7  3  6]\n",
      " [ 4  3  1  2]\n",
      " [19 22 17 15]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.33      0.25        21\n",
      "           1       0.17      0.33      0.23        21\n",
      "           2       0.05      0.10      0.06        10\n",
      "           3       0.54      0.21      0.30        73\n",
      "\n",
      "    accuracy                           0.24       125\n",
      "   macro avg       0.24      0.24      0.21       125\n",
      "weighted avg       0.38      0.24      0.26       125\n",
      "\n",
      "Balanced Accuracy Score: 0.24303652968036527\n",
      "Accuracy Score: 0.24\n",
      "Results for learning_rate=1e-05, weight_decay=0.1\n",
      "Train Loss: 2.3351609309514365\n",
      "Evaluation Metrics: {'eval_loss': 2.605074882507324, 'eval_pearson': 0.03968428210805165, 'eval_runtime': 56.1821, 'eval_samples_per_second': 2.225, 'eval_steps_per_second': 0.036, 'epoch': 3.0}\n",
      "Performance Metrics: None\n",
      "\n",
      "Training with learning_rate=5e-05, weight_decay=0.001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1897378/486941468.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n",
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/24 34:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.352450</td>\n",
       "      <td>0.021218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.103200</td>\n",
       "      <td>2.136938</td>\n",
       "      <td>0.063233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.799200</td>\n",
       "      <td>2.089119</td>\n",
       "      <td>0.055747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1897378/486941468.py:159: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparing test 249    0\n",
      "558    3\n",
      "174    1\n",
      "280    0\n",
      "110    1\n",
      "      ..\n",
      "6      3\n",
      "104    3\n",
      "114    3\n",
      "355    3\n",
      "132    3\n",
      "Name: Institutional_Form_category, Length: 125, dtype: int8 and pred 249    3\n",
      "558    2\n",
      "174    3\n",
      "280    2\n",
      "110    2\n",
      "      ..\n",
      "6      3\n",
      "104    0\n",
      "114    3\n",
      "355    1\n",
      "132    3\n",
      "Name: predictions, Length: 125, dtype: int64\n",
      "Confusion Matrix:\n",
      "[[ 7  5  4  5]\n",
      " [ 4  7  6  4]\n",
      " [ 3  3  2  2]\n",
      " [20 14 20 19]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.33      0.25        21\n",
      "           1       0.24      0.33      0.28        21\n",
      "           2       0.06      0.20      0.10        10\n",
      "           3       0.63      0.26      0.37        73\n",
      "\n",
      "    accuracy                           0.28       125\n",
      "   macro avg       0.29      0.28      0.25       125\n",
      "weighted avg       0.45      0.28      0.31       125\n",
      "\n",
      "Balanced Accuracy Score: 0.2817351598173516\n",
      "Accuracy Score: 0.28\n",
      "Results for learning_rate=5e-05, weight_decay=0.001\n",
      "Train Loss: 1.9240225950876872\n",
      "Evaluation Metrics: {'eval_loss': 2.0891194343566895, 'eval_pearson': 0.05574748494716618, 'eval_runtime': 56.3458, 'eval_samples_per_second': 2.218, 'eval_steps_per_second': 0.035, 'epoch': 3.0}\n",
      "Performance Metrics: None\n",
      "\n",
      "Training with learning_rate=5e-05, weight_decay=0.01\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1897378/486941468.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n",
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/24 33:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.028989</td>\n",
       "      <td>0.092120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.531700</td>\n",
       "      <td>1.851082</td>\n",
       "      <td>0.259632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.250300</td>\n",
       "      <td>1.819140</td>\n",
       "      <td>0.259766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "SafetensorError",
     "evalue": "Error while serializing: IoError(Os { code: 122, kind: FilesystemQuotaExceeded, message: \"Disk quota exceeded\" })",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSafetensorError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 272\u001b[0m\n\u001b[1;32m    260\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomTrainer(\n\u001b[1;32m    261\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    262\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m     class_weights\u001b[38;5;241m=\u001b[39mclass_weights,\n\u001b[1;32m    269\u001b[0m )\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 272\u001b[0m train_result \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# Record training loss and evaluation metrics\u001b[39;00m\n\u001b[1;32m    275\u001b[0m eval_metrics \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   2124\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   2125\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   2126\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   2127\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   2128\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/site-packages/transformers/trainer.py:2573\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2572\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2573\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\n\u001b[1;32m   2575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   2576\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[1;32m   2577\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/site-packages/transformers/trainer.py:3007\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   3004\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate(trial, ignore_keys_for_eval)\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 3007\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_checkpoint(model, trial, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n\u001b[1;32m   3008\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/site-packages/transformers/trainer.py:3097\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   3095\u001b[0m run_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_output_dir(trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m   3096\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(run_dir, checkpoint_folder)\n\u001b[0;32m-> 3097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(output_dir, _internal_call\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   3099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_only_model:\n\u001b[1;32m   3100\u001b[0m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[1;32m   3101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_optimizer_and_scheduler(output_dir)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/site-packages/transformers/trainer.py:3730\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[0;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[1;32m   3727\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped\u001b[38;5;241m.\u001b[39msave_checkpoint(output_dir)\n\u001b[1;32m   3729\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 3730\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save(output_dir)\n\u001b[1;32m   3732\u001b[0m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[1;32m   3733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/site-packages/transformers/trainer.py:3834\u001b[0m, in \u001b[0;36mTrainer._save\u001b[0;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[1;32m   3832\u001b[0m             torch\u001b[38;5;241m.\u001b[39msave(state_dict, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, WEIGHTS_NAME))\n\u001b[1;32m   3833\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3834\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_pretrained(\n\u001b[1;32m   3835\u001b[0m         output_dir, state_dict\u001b[38;5;241m=\u001b[39mstate_dict, safe_serialization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_safetensors\n\u001b[1;32m   3836\u001b[0m     )\n\u001b[1;32m   3838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessing_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3839\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessing_class\u001b[38;5;241m.\u001b[39msave_pretrained(output_dir)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/site-packages/peft/peft_model.py:369\u001b[0m, in \u001b[0;36mPeftModel.save_pretrained\u001b[0;34m(self, save_directory, safe_serialization, selected_adapters, save_embedding_layers, is_main_process, convert_pissa_to_lora, path_initial_model_for_weight_conversion, **kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m         peft_config\u001b[38;5;241m.\u001b[39msave_pretrained(path_initial_model_for_weight_conversion)\n\u001b[1;32m    366\u001b[0m         output_state_dict \u001b[38;5;241m=\u001b[39m save_mutated_as_lora(\n\u001b[1;32m    367\u001b[0m             peft_config, path_initial_model_for_weight_conversion, output_state_dict, kwargs\n\u001b[1;32m    368\u001b[0m         )\n\u001b[0;32m--> 369\u001b[0m     safe_save_file(\n\u001b[1;32m    370\u001b[0m         output_state_dict,\n\u001b[1;32m    371\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, SAFETENSORS_WEIGHTS_NAME),\n\u001b[1;32m    372\u001b[0m         metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    373\u001b[0m     )\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_main_process:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m path_initial_model_for_weight_conversion \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.11/site-packages/safetensors/torch.py:286\u001b[0m, in \u001b[0;36msave_file\u001b[0;34m(tensors, filename, metadata)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_file\u001b[39m(\n\u001b[1;32m    256\u001b[0m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[1;32m    257\u001b[0m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    258\u001b[0m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m ):\n\u001b[1;32m    260\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;124;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m     serialize_file(_flatten(tensors), filename, metadata\u001b[38;5;241m=\u001b[39mmetadata)\n",
      "\u001b[0;31mSafetensorError\u001b[0m: Error while serializing: IoError(Os { code: 122, kind: FilesystemQuotaExceeded, message: \"Disk quota exceeded\" })"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import functools\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import evaluate\n",
    "import re\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, balanced_accuracy_score, accuracy_score\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from datasets import Dataset, DatasetDict\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "df = pd.read_csv('filtered_labeled.csv')\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'(\\n[a-zA-Z])', '', text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    return cleaned_text\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "df['Institutional_Form'] = df['Institutional_Form'].astype('category')\n",
    "df['Institutional_Form_category'] = df['Institutional_Form'].cat.codes\n",
    "\n",
    "category_map = {code: category for code, category in enumerate(df['Institutional_Form'].cat.categories)}\n",
    "\n",
    "df_train, df_val = train_test_split(df, train_size=0.8, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "def generate_features_with_prompt(df):\n",
    "    # Define the instruction\n",
    "    instruction = (\n",
    "        \"You are a contract classification assistant. Your task is to classify the contract text \"\n",
    "        \"into one of the predefined categories. Here are the criteria for each category:\\n\"\n",
    "        \"- Joint Operations: Partnership arrangements to jointly produce services with one or more organizations.\\n\"\n",
    "        \"- New Joint Entities: Two or more organizations creating a separate new entity to manage or govern a shared asset or service.\\n\"\n",
    "        \"- Resource Sharing: Sharing of information, personnel, equipment, etc., between governments or community organizations to provide services.\\n\"\n",
    "        \"- Service Contracts: Agreements with outside entities, public or private, for provision or support services.\\n\"\n",
    "        \"Analyze the given text carefully and respond with the appropriate category.\"\n",
    "    )\n",
    "    df['Institutional_Form'] = df['Institutional_Form'].astype(str)\n",
    "    # Generate the `input` column by combining instruction, text, and category\n",
    "    df['input'] = (\n",
    "        instruction\n",
    "        + \"\\n\\nContract Text: \"\n",
    "        + df['text']\n",
    "        + \"\\n\\nInstitutional Form Category: \"\n",
    "        + df['Institutional_Form']\n",
    "    )\n",
    "\n",
    "    # Add the numerical category column if not already present\n",
    "    if 'Institutional_Form_category' not in df.columns:\n",
    "        df['Institutional_Form'] = df['Institutional_Form'].astype('category')\n",
    "        df['Institutional_Form_category'] = df['Institutional_Form'].cat.codes\n",
    "\n",
    "    return df\n",
    "\n",
    "generate_features_with_prompt(df_train)\n",
    "generate_features_with_prompt(df_val)\n",
    "\n",
    "dataset_train = Dataset.from_pandas(df_train.reset_index(drop=True))\n",
    "dataset_val = Dataset.from_pandas(df_val.reset_index(drop=True))\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': dataset_train,\n",
    "    'val': dataset_val,\n",
    "})\n",
    "\n",
    "#Since our classes are not balanced let's calculate class weights based on inverse value counts\n",
    "#Convert to pytorch tensor since we will need it\n",
    "df_train.Institutional_Form_category.value_counts(normalize=True)\n",
    "class_weights=(1/df_train.Institutional_Form.value_counts(normalize=True).sort_index()).tolist()\n",
    "class_weights=torch.tensor(class_weights)\n",
    "class_weights=class_weights/class_weights.sum()\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "#Quantization Config (for QLORA)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True, # enable 4-bit quantization\n",
    "    bnb_4bit_quant_type = 'nf4', # information theoretically optimal dtype for normally distributed weights\n",
    "    bnb_4bit_use_double_quant = True, # quantize quantized weights //insert xzibit meme\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16 # optimized fp format for ML\n",
    ")\n",
    "\n",
    "#\n",
    "lora_config = LoraConfig(\n",
    "    r = 16, # the dimension of the low-rank matrices\n",
    "    lora_alpha = 8, # scaling factor for LoRA activations vs pre-trained weight activations\n",
    "    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "    lora_dropout = 0.05, # dropout probability of the LoRA layers\n",
    "    bias = 'none', # wether to train bias weights, set to 'none' for attention layers\n",
    "    task_type = 'SEQ_CLS'\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    num_labels=len(category_map)\n",
    ")\n",
    "\n",
    "#prepare_model_for_kbit_training() function to preprocess the quantized model for training.\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "#Must use .cache = False as below or it crashes from my experience\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "def llama_preprocessing_function(examples):\n",
    "    return tokenizer(examples['input'], truncation=True, max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(llama_preprocessing_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"Institutional_Form_category\", \"label\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "#Purpose: Automatically pads text data to the longest sequence in a batch\n",
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    try:\n",
    "        # it's a classification task, take the argmax\n",
    "        predictions_processed = np.argmax(predictions, axis=1)\n",
    "\n",
    "        # Calculate Pearson correlation\n",
    "        pearson, _ = pearsonr(predictions_processed, labels)\n",
    "\n",
    "        return {'pearson': pearson}\n",
    "    except Exception as e:\n",
    "        print(f\"Error in compute_metrics: {e}\")\n",
    "        return {'pearson': None}\n",
    "    \n",
    "#We will have a custom loss function that deals with the class weights and have class weights as additional argument in constructor\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Ensure label_weights is a tensor\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, num_items_in_batch=None, return_outputs=False):\n",
    "        # Extract labels and convert them to long type for cross_entropy\n",
    "        labels = inputs.pop(\"labels\").long()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Extract logits assuming they are directly outputted by the model\n",
    "        logits = outputs.get('logits')\n",
    "\n",
    "        # Compute custom loss with class weights for imbalanced data handling\n",
    "        if self.class_weights is not None:\n",
    "            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "\n",
    "\n",
    "def make_predictions(model, df):\n",
    "    \n",
    "\n",
    "    sentences = df.input.tolist()\n",
    "\n",
    "      # Define the batch size\n",
    "    batch_size = 32  # You can adjust this based on your system's memory capacity\n",
    "\n",
    "      # Initialize an empty list to store the model outputs\n",
    "    all_outputs = []\n",
    "\n",
    "      # Process the sentences in batches\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "          # Get the batch of sentences\n",
    "        batch_sentences = sentences[i:i + batch_size]\n",
    "\n",
    "          # Tokenize the batch\n",
    "        inputs = tokenizer(batch_sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "          # Move tensors to the device where the model is (e.g., GPU or CPU)\n",
    "        inputs = {k: v.to('cuda' if torch.cuda.is_available() else 'cpu') for k, v in inputs.items()}\n",
    "\n",
    "          # Perform inference and store the logits\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            all_outputs.append(outputs['logits'])\n",
    "\n",
    "    final_outputs = torch.cat(all_outputs, dim=0)\n",
    "    df['predictions']=final_outputs.argmax(axis=1).cpu().numpy()\n",
    "    #df['predictions']=df['predictions'].apply(lambda l:category_map[l])\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_performance_metrics(df):\n",
    "    y_test = df.Institutional_Form_category\n",
    "    y_pred = df.predictions\n",
    "    print(f\"comparing test {y_test} and pred {y_pred}\")\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(\"Balanced Accuracy Score:\", balanced_accuracy_score(y_test, y_pred))\n",
    "    print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "    \n",
    "# Define hyperparameter grid\n",
    "learning_rates = [1e-5, 5e-5, 1e-4]\n",
    "weight_decays = [0.001, 0.01, 0.1]\n",
    "\n",
    "# Keep track of results\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for wd in weight_decays:\n",
    "        print(f\"\\nTraining with learning_rate={lr}, weight_decay={wd}\\n\")\n",
    "\n",
    "        # Update training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f'sequence_classification_lr{lr}_wd{wd}',\n",
    "            learning_rate=lr,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=3,\n",
    "            weight_decay=wd,\n",
    "            evaluation_strategy='epoch',\n",
    "            save_strategy='epoch',\n",
    "            load_best_model_at_end=True,\n",
    "            logging_dir=f'logs_lr{lr}_wd{wd}',\n",
    "            logging_steps=10\n",
    "        )\n",
    "\n",
    "        # Initialize trainer\n",
    "        trainer = CustomTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets['train'],\n",
    "            eval_dataset=tokenized_datasets['val'],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=collate_fn,\n",
    "            compute_metrics=compute_metrics,\n",
    "            class_weights=class_weights,\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        train_result = trainer.train()\n",
    "\n",
    "        # Record training loss and evaluation metrics\n",
    "        eval_metrics = trainer.evaluate()\n",
    "        train_loss = train_result.training_loss\n",
    "\n",
    "        # Make predictions and compute performance metrics\n",
    "        df_val = make_predictions(model, df_val)\n",
    "        performance_metrics = get_performance_metrics(df_val)\n",
    "\n",
    "        # Append the metrics to results\n",
    "        results.append({\n",
    "            \"learning_rate\": lr,\n",
    "            \"weight_decay\": wd,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"eval_metrics\": eval_metrics,\n",
    "            \"performance_metrics\": performance_metrics\n",
    "        })\n",
    "\n",
    "        # Print results for the current model\n",
    "        print(f\"Results for learning_rate={lr}, weight_decay={wd}\")\n",
    "        print(f\"Train Loss: {train_loss}\")\n",
    "        print(f\"Evaluation Metrics: {eval_metrics}\")\n",
    "        print(f\"Performance Metrics: {performance_metrics}\")\n",
    "\n",
    "# Summarize all results\n",
    "print(\"\\nSummary of all results:\")\n",
    "for res in results:\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e47788c2-2600-4562-9f09-50a234bd859a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.10s/it]\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.1-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 498/498 [00:00<00:00, 1642.46 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:00<00:00, 1594.17 examples/s]\n",
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_2117250/2533371138.py:159: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with learning_rate=0.0002, weight_decay=0.01\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2117250/2533371138.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmghasemizade97\u001b[0m (\u001b[33mmghasemizade97-university-of-vermont\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/u/mghasemizade/wandb/run-20241213_065115-bg5apodu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mghasemizade97-university-of-vermont/huggingface/runs/bg5apodu' target=\"_blank\">sequence_classification_lr0.0002_wd0.01</a></strong> to <a href='https://wandb.ai/mghasemizade97-university-of-vermont/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mghasemizade97-university-of-vermont/huggingface' target=\"_blank\">https://wandb.ai/mghasemizade97-university-of-vermont/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mghasemizade97-university-of-vermont/huggingface/runs/bg5apodu' target=\"_blank\">https://wandb.ai/mghasemizade97-university-of-vermont/huggingface/runs/bg5apodu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/24 34:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.862094</td>\n",
       "      <td>0.214477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.093500</td>\n",
       "      <td>1.646135</td>\n",
       "      <td>0.181588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.479100</td>\n",
       "      <td>1.569773</td>\n",
       "      <td>0.294377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_2117250/2533371138.py:159: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparing test 249    0\n",
      "558    3\n",
      "174    1\n",
      "280    0\n",
      "110    1\n",
      "      ..\n",
      "6      3\n",
      "104    3\n",
      "114    3\n",
      "355    3\n",
      "132    3\n",
      "Name: Institutional_Form_category, Length: 125, dtype: int8 and pred 249    1\n",
      "558    1\n",
      "174    0\n",
      "280    1\n",
      "110    1\n",
      "      ..\n",
      "6      1\n",
      "104    2\n",
      "114    3\n",
      "355    0\n",
      "132    1\n",
      "Name: predictions, Length: 125, dtype: int64\n",
      "Confusion Matrix:\n",
      "[[10  6  2  3]\n",
      " [ 6 11  3  1]\n",
      " [ 0  7  2  1]\n",
      " [13 25 11 24]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.48      0.40        21\n",
      "           1       0.22      0.52      0.31        21\n",
      "           2       0.11      0.20      0.14        10\n",
      "           3       0.83      0.33      0.47        73\n",
      "\n",
      "    accuracy                           0.38       125\n",
      "   macro avg       0.38      0.38      0.33       125\n",
      "weighted avg       0.59      0.38      0.41       125\n",
      "\n",
      "Balanced Accuracy Score: 0.3821917808219178\n",
      "Accuracy Score: 0.376\n",
      "Results for learning_rate=0.0002, weight_decay=0.01\n",
      "Train Loss: 1.6977777083714802\n",
      "Evaluation Metrics: {'eval_loss': 1.5697726011276245, 'eval_pearson': 0.2943766656005982, 'eval_runtime': 56.194, 'eval_samples_per_second': 2.224, 'eval_steps_per_second': 0.036, 'epoch': 3.0}\n",
      "Performance Metrics: None\n",
      "\n",
      "Training with learning_rate=0.0002, weight_decay=0.1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2117250/2533371138.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n",
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/24 34:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.416458</td>\n",
       "      <td>0.302279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.158900</td>\n",
       "      <td>1.331382</td>\n",
       "      <td>0.412799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.668400</td>\n",
       "      <td>1.206698</td>\n",
       "      <td>0.390260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_2117250/2533371138.py:159: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparing test 249    0\n",
      "558    3\n",
      "174    1\n",
      "280    0\n",
      "110    1\n",
      "      ..\n",
      "6      3\n",
      "104    3\n",
      "114    3\n",
      "355    3\n",
      "132    3\n",
      "Name: Institutional_Form_category, Length: 125, dtype: int8 and pred 249    2\n",
      "558    3\n",
      "174    1\n",
      "280    0\n",
      "110    3\n",
      "      ..\n",
      "6      3\n",
      "104    2\n",
      "114    3\n",
      "355    0\n",
      "132    3\n",
      "Name: predictions, Length: 125, dtype: int64\n",
      "Confusion Matrix:\n",
      "[[10  4  3  4]\n",
      " [ 2 13  3  3]\n",
      " [ 0  2  6  2]\n",
      " [12  7 11 43]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.48      0.44        21\n",
      "           1       0.50      0.62      0.55        21\n",
      "           2       0.26      0.60      0.36        10\n",
      "           3       0.83      0.59      0.69        73\n",
      "\n",
      "    accuracy                           0.58       125\n",
      "   macro avg       0.50      0.57      0.51       125\n",
      "weighted avg       0.66      0.58      0.60       125\n",
      "\n",
      "Balanced Accuracy Score: 0.5710697977821266\n",
      "Accuracy Score: 0.576\n",
      "Results for learning_rate=0.0002, weight_decay=0.1\n",
      "Train Loss: 0.8450728158156077\n",
      "Evaluation Metrics: {'eval_loss': 1.2066984176635742, 'eval_pearson': 0.39025964061723534, 'eval_runtime': 56.488, 'eval_samples_per_second': 2.213, 'eval_steps_per_second': 0.035, 'epoch': 3.0}\n",
      "Performance Metrics: None\n",
      "\n",
      "Training with learning_rate=0.0002, weight_decay=0.2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2117250/2533371138.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n",
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/24 34:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.334070</td>\n",
       "      <td>0.431341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.522500</td>\n",
       "      <td>1.542571</td>\n",
       "      <td>0.461159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.244200</td>\n",
       "      <td>1.386057</td>\n",
       "      <td>0.495781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparing test 249    0\n",
      "558    3\n",
      "174    1\n",
      "280    0\n",
      "110    1\n",
      "      ..\n",
      "6      3\n",
      "104    3\n",
      "114    3\n",
      "355    3\n",
      "132    3\n",
      "Name: Institutional_Form_category, Length: 125, dtype: int8 and pred 249    2\n",
      "558    3\n",
      "174    1\n",
      "280    0\n",
      "110    3\n",
      "      ..\n",
      "6      0\n",
      "104    2\n",
      "114    3\n",
      "355    3\n",
      "132    3\n",
      "Name: predictions, Length: 125, dtype: int64\n",
      "Confusion Matrix:\n",
      "[[11  4  5  1]\n",
      " [ 1 12  4  4]\n",
      " [ 0  1  6  3]\n",
      " [13  4 13 43]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.52      0.48        21\n",
      "           1       0.57      0.57      0.57        21\n",
      "           2       0.21      0.60      0.32        10\n",
      "           3       0.84      0.59      0.69        73\n",
      "\n",
      "    accuracy                           0.58       125\n",
      "   macro avg       0.52      0.57      0.51       125\n",
      "weighted avg       0.68      0.58      0.61       125\n",
      "\n",
      "Balanced Accuracy Score: 0.5710697977821266\n",
      "Accuracy Score: 0.576\n",
      "Results for learning_rate=0.0002, weight_decay=0.2\n",
      "Train Loss: 0.34373274942239124\n",
      "Evaluation Metrics: {'eval_loss': 1.3340703248977661, 'eval_pearson': 0.43134100666315606, 'eval_runtime': 56.6629, 'eval_samples_per_second': 2.206, 'eval_steps_per_second': 0.035, 'epoch': 3.0}\n",
      "Performance Metrics: None\n",
      "\n",
      "Summary of all results:\n",
      "{'learning_rate': 0.0002, 'weight_decay': 0.01, 'train_loss': 1.6977777083714802, 'eval_metrics': {'eval_loss': 1.5697726011276245, 'eval_pearson': 0.2943766656005982, 'eval_runtime': 56.194, 'eval_samples_per_second': 2.224, 'eval_steps_per_second': 0.036, 'epoch': 3.0}, 'performance_metrics': None}\n",
      "{'learning_rate': 0.0002, 'weight_decay': 0.1, 'train_loss': 0.8450728158156077, 'eval_metrics': {'eval_loss': 1.2066984176635742, 'eval_pearson': 0.39025964061723534, 'eval_runtime': 56.488, 'eval_samples_per_second': 2.213, 'eval_steps_per_second': 0.035, 'epoch': 3.0}, 'performance_metrics': None}\n",
      "{'learning_rate': 0.0002, 'weight_decay': 0.2, 'train_loss': 0.34373274942239124, 'eval_metrics': {'eval_loss': 1.3340703248977661, 'eval_pearson': 0.43134100666315606, 'eval_runtime': 56.6629, 'eval_samples_per_second': 2.206, 'eval_steps_per_second': 0.035, 'epoch': 3.0}, 'performance_metrics': None}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import functools\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import evaluate\n",
    "import re\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, balanced_accuracy_score, accuracy_score\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from datasets import Dataset, DatasetDict\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "df = pd.read_csv('filtered_labeled.csv')\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'(\\n[a-zA-Z])', '', text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    return cleaned_text\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "df['Institutional_Form'] = df['Institutional_Form'].astype('category')\n",
    "df['Institutional_Form_category'] = df['Institutional_Form'].cat.codes\n",
    "\n",
    "category_map = {code: category for code, category in enumerate(df['Institutional_Form'].cat.categories)}\n",
    "\n",
    "df_train, df_val = train_test_split(df, train_size=0.8, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "def generate_features_with_prompt(df):\n",
    "    # Define the instruction\n",
    "    instruction = (\n",
    "        \"You are a contract classification assistant. Your task is to classify the contract text \"\n",
    "        \"into one of the predefined categories. Here are the criteria for each category:\\n\"\n",
    "        \"- Joint Operations: Partnership arrangements to jointly produce services with one or more organizations.\\n\"\n",
    "        \"- New Joint Entities: Two or more organizations creating a separate new entity to manage or govern a shared asset or service.\\n\"\n",
    "        \"- Resource Sharing: Sharing of information, personnel, equipment, etc., between governments or community organizations to provide services.\\n\"\n",
    "        \"- Service Contracts: Agreements with outside entities, public or private, for provision or support services.\\n\"\n",
    "        \"Analyze the given text carefully and respond with the appropriate category.\"\n",
    "    )\n",
    "    df['Institutional_Form'] = df['Institutional_Form'].astype(str)\n",
    "    # Generate the `input` column by combining instruction, text, and category\n",
    "    df['input'] = (\n",
    "        instruction\n",
    "        + \"\\n\\nContract Text: \"\n",
    "        + df['text']\n",
    "        + \"\\n\\nInstitutional Form Category: \"\n",
    "        + df['Institutional_Form']\n",
    "    )\n",
    "\n",
    "    # Add the numerical category column if not already present\n",
    "    if 'Institutional_Form_category' not in df.columns:\n",
    "        df['Institutional_Form'] = df['Institutional_Form'].astype('category')\n",
    "        df['Institutional_Form_category'] = df['Institutional_Form'].cat.codes\n",
    "\n",
    "    return df\n",
    "\n",
    "generate_features_with_prompt(df_train)\n",
    "generate_features_with_prompt(df_val)\n",
    "\n",
    "dataset_train = Dataset.from_pandas(df_train.reset_index(drop=True))\n",
    "dataset_val = Dataset.from_pandas(df_val.reset_index(drop=True))\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': dataset_train,\n",
    "    'val': dataset_val,\n",
    "})\n",
    "\n",
    "#Since our classes are not balanced let's calculate class weights based on inverse value counts\n",
    "#Convert to pytorch tensor since we will need it\n",
    "df_train.Institutional_Form_category.value_counts(normalize=True)\n",
    "class_weights=(1/df_train.Institutional_Form.value_counts(normalize=True).sort_index()).tolist()\n",
    "class_weights=torch.tensor(class_weights)\n",
    "class_weights=class_weights/class_weights.sum()\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "#Quantization Config (for QLORA)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True, # enable 4-bit quantization\n",
    "    bnb_4bit_quant_type = 'nf4', # information theoretically optimal dtype for normally distributed weights\n",
    "    bnb_4bit_use_double_quant = True, # quantize quantized weights //insert xzibit meme\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16 # optimized fp format for ML\n",
    ")\n",
    "\n",
    "#\n",
    "lora_config = LoraConfig(\n",
    "    r = 16, # the dimension of the low-rank matrices\n",
    "    lora_alpha = 8, # scaling factor for LoRA activations vs pre-trained weight activations\n",
    "    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "    lora_dropout = 0.05, # dropout probability of the LoRA layers\n",
    "    bias = 'none', # wether to train bias weights, set to 'none' for attention layers\n",
    "    task_type = 'SEQ_CLS'\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    num_labels=len(category_map)\n",
    ")\n",
    "\n",
    "#prepare_model_for_kbit_training() function to preprocess the quantized model for training.\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "#Must use .cache = False as below or it crashes from my experience\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "def llama_preprocessing_function(examples):\n",
    "    return tokenizer(examples['input'], truncation=True, max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(llama_preprocessing_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"Institutional_Form_category\", \"label\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "#Purpose: Automatically pads text data to the longest sequence in a batch\n",
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    try:\n",
    "        # it's a classification task, take the argmax\n",
    "        predictions_processed = np.argmax(predictions, axis=1)\n",
    "\n",
    "        # Calculate Pearson correlation\n",
    "        pearson, _ = pearsonr(predictions_processed, labels)\n",
    "\n",
    "        return {'pearson': pearson}\n",
    "    except Exception as e:\n",
    "        print(f\"Error in compute_metrics: {e}\")\n",
    "        return {'pearson': None}\n",
    "    \n",
    "#We will have a custom loss function that deals with the class weights and have class weights as additional argument in constructor\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Ensure label_weights is a tensor\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, num_items_in_batch=None, return_outputs=False):\n",
    "        # Extract labels and convert them to long type for cross_entropy\n",
    "        labels = inputs.pop(\"labels\").long()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Extract logits assuming they are directly outputted by the model\n",
    "        logits = outputs.get('logits')\n",
    "\n",
    "        # Compute custom loss with class weights for imbalanced data handling\n",
    "        if self.class_weights is not None:\n",
    "            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "\n",
    "\n",
    "def make_predictions(model, df):\n",
    "    \n",
    "\n",
    "    sentences = df.input.tolist()\n",
    "\n",
    "      # Define the batch size\n",
    "    batch_size = 32  # You can adjust this based on your system's memory capacity\n",
    "\n",
    "      # Initialize an empty list to store the model outputs\n",
    "    all_outputs = []\n",
    "\n",
    "      # Process the sentences in batches\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "          # Get the batch of sentences\n",
    "        batch_sentences = sentences[i:i + batch_size]\n",
    "\n",
    "          # Tokenize the batch\n",
    "        inputs = tokenizer(batch_sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "          # Move tensors to the device where the model is (e.g., GPU or CPU)\n",
    "        inputs = {k: v.to('cuda' if torch.cuda.is_available() else 'cpu') for k, v in inputs.items()}\n",
    "\n",
    "          # Perform inference and store the logits\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            all_outputs.append(outputs['logits'])\n",
    "\n",
    "    final_outputs = torch.cat(all_outputs, dim=0)\n",
    "    df['predictions']=final_outputs.argmax(axis=1).cpu().numpy()\n",
    "    #df['predictions']=df['predictions'].apply(lambda l:category_map[l])\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_performance_metrics(df):\n",
    "    y_test = df.Institutional_Form_category\n",
    "    y_pred = df.predictions\n",
    "    print(f\"comparing test {y_test} and pred {y_pred}\")\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(\"Balanced Accuracy Score:\", balanced_accuracy_score(y_test, y_pred))\n",
    "    print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "    \n",
    "# Define hyperparameter grid\n",
    "learning_rates = [2e-4]\n",
    "weight_decays = [0.01, 0.1, 0.2]\n",
    "\n",
    "# Keep track of results\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for wd in weight_decays:\n",
    "        print(f\"\\nTraining with learning_rate={lr}, weight_decay={wd}\\n\")\n",
    "\n",
    "        # Update training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f'sequence_classification_lr{lr}_wd{wd}',\n",
    "            learning_rate=lr,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=3,\n",
    "            weight_decay=wd,\n",
    "            evaluation_strategy='epoch',\n",
    "            save_strategy='epoch',\n",
    "            load_best_model_at_end=True,\n",
    "            logging_dir=f'logs_lr{lr}_wd{wd}',\n",
    "            logging_steps=10\n",
    "        )\n",
    "\n",
    "        # Initialize trainer\n",
    "        trainer = CustomTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets['train'],\n",
    "            eval_dataset=tokenized_datasets['val'],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=collate_fn,\n",
    "            compute_metrics=compute_metrics,\n",
    "            class_weights=class_weights,\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        train_result = trainer.train()\n",
    "\n",
    "        # Record training loss and evaluation metrics\n",
    "        eval_metrics = trainer.evaluate()\n",
    "        train_loss = train_result.training_loss\n",
    "\n",
    "        # Make predictions and compute performance metrics\n",
    "        df_val = make_predictions(model, df_val)\n",
    "        performance_metrics = get_performance_metrics(df_val)\n",
    "\n",
    "        # Append the metrics to results\n",
    "        results.append({\n",
    "            \"learning_rate\": lr,\n",
    "            \"weight_decay\": wd,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"eval_metrics\": eval_metrics,\n",
    "            \"performance_metrics\": performance_metrics\n",
    "        })\n",
    "\n",
    "        # Print results for the current model\n",
    "        print(f\"Results for learning_rate={lr}, weight_decay={wd}\")\n",
    "        print(f\"Train Loss: {train_loss}\")\n",
    "        print(f\"Evaluation Metrics: {eval_metrics}\")\n",
    "        print(f\"Performance Metrics: {performance_metrics}\")\n",
    "\n",
    "# Summarize all results\n",
    "print(\"\\nSummary of all results:\")\n",
    "for res in results:\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6527bd28-c0ed-42b1-839f-d6858a555b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.16s/it]\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.1-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436/436 [00:00<00:00, 1715.37 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 93/93 [00:00<00:00, 1739.14 examples/s]\n",
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_2117250/2270156896.py:163: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/tmp/ipykernel_2117250/2270156896.py:166: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n",
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 50:40, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.466296</td>\n",
       "      <td>0.134326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.928252</td>\n",
       "      <td>0.393512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.875833</td>\n",
       "      <td>0.577480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.508594</td>\n",
       "      <td>0.389230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.662429</td>\n",
       "      <td>0.472553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "make_predictions() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 260\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBalanced Accuracy Score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, balanced_accuracy_score(y_test, y_pred))\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy Score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_score(y_test, y_pred))\n\u001b[0;32m--> 260\u001b[0m df_val \u001b[38;5;241m=\u001b[39m make_predictions(model, tokenizer, df_val)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m#make_predictions(model,df_val)\u001b[39;00m\n\u001b[1;32m    264\u001b[0m get_performance_metrics(df_val)\n",
      "\u001b[0;31mTypeError\u001b[0m: make_predictions() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import functools\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import evaluate\n",
    "import re\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, balanced_accuracy_score, accuracy_score\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from datasets import Dataset, DatasetDict\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "df = pd.read_csv('filtered_labeled.csv')\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'(\\n[a-zA-Z])', '', text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    return cleaned_text\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "df['Institutional_Form'] = df['Institutional_Form'].astype('category')\n",
    "df['Institutional_Form_category'] = df['Institutional_Form'].cat.codes\n",
    "category_map = {code: category for code, category in enumerate(df['Institutional_Form'].cat.categories)}\n",
    "\n",
    "#df_train, df_val = train_test_split(df, train_size=0.8, test_size=0.2, random_state=42)\n",
    "train_size = 0.7\n",
    "test_size = 0.15\n",
    "val_size = 0.15\n",
    "\n",
    "df_train, df_temp = train_test_split(df, train_size=train_size, random_state=42)\n",
    "df_val, df_test = train_test_split(df_temp, test_size=test_size / (test_size + val_size), random_state=42)\n",
    "\n",
    "def generate_features_with_prompt(df):\n",
    "    # Define the instruction\n",
    "    instruction = (\n",
    "        \"You are a contract classification assistant. Your task is to classify the contract text \"\n",
    "        \"into one of the predefined categories. Here are the criteria for each category:\\n\"\n",
    "        \"- Joint Operations: Partnership arrangements to jointly produce services with one or more organizations.\\n\"\n",
    "        \"- New Joint Entities: Two or more organizations creating a separate new entity to manage or govern a shared asset or service.\\n\"\n",
    "        \"- Resource Sharing: Sharing of information, personnel, equipment, etc., between governments or community organizations to provide services.\\n\"\n",
    "        \"- Service Contracts: Agreements with outside entities, public or private, for provision or support services.\\n\"\n",
    "        \"Analyze the given text carefully and respond with the appropriate category.\"\n",
    "    )\n",
    "    df['Institutional_Form'] = df['Institutional_Form'].astype(str)\n",
    "    # Generate the `input` column by combining instruction, text, and category\n",
    "    df['input'] = (\n",
    "        instruction\n",
    "        + \"\\n\\nContract Text: \"\n",
    "        + df['text']\n",
    "        + \"\\n\\nInstitutional Form Category: \"\n",
    "        + df['Institutional_Form']\n",
    "    )\n",
    "\n",
    "    # Add the numerical category column if not already present\n",
    "    if 'Institutional_Form_category' not in df.columns:\n",
    "        df['Institutional_Form'] = df['Institutional_Form'].astype('category')\n",
    "        df['Institutional_Form_category'] = df['Institutional_Form'].cat.codes\n",
    "\n",
    "    return df\n",
    "\n",
    "generate_features_with_prompt(df_train)\n",
    "generate_features_with_prompt(df_val)\n",
    "\n",
    "dataset_train = Dataset.from_pandas(df_train.reset_index(drop=True))\n",
    "dataset_val = Dataset.from_pandas(df_val.reset_index(drop=True))\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': dataset_train,\n",
    "    'val': dataset_val,\n",
    "})\n",
    "\n",
    "#Since our classes are not balanced let's calculate class weights based on inverse value counts\n",
    "#Convert to pytorch tensor since we will need it\n",
    "df_train.Institutional_Form_category.value_counts(normalize=True)\n",
    "class_weights=(1/df_train.Institutional_Form.value_counts(normalize=True).sort_index()).tolist()\n",
    "class_weights=torch.tensor(class_weights)\n",
    "class_weights=class_weights/class_weights.sum()\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "#Quantization Config (for QLORA)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True, # enable 4-bit quantization\n",
    "    bnb_4bit_quant_type = 'nf4', # information theoretically optimal dtype for normally distributed weights\n",
    "    bnb_4bit_use_double_quant = True, # quantize quantized weights //insert xzibit meme\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16 # optimized fp format for ML\n",
    ")\n",
    "\n",
    "#\n",
    "lora_config = LoraConfig(\n",
    "    r = 16, # the dimension of the low-rank matrices\n",
    "    lora_alpha = 8, # scaling factor for LoRA activations vs pre-trained weight activations\n",
    "    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "    lora_dropout = 0.05, # dropout probability of the LoRA layers\n",
    "    bias = 'none', # wether to train bias weights, set to 'none' for attention layers\n",
    "    task_type = 'SEQ_CLS'\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    num_labels=len(category_map)\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "#Must use .cache = False as below or it crashes from my experience\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "def llama_preprocessing_function(examples):\n",
    "    return tokenizer(examples['input'], truncation=True, max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(llama_preprocessing_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"Institutional_Form_category\", \"label\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "#Purpose: Automatically pads text data to the longest sequence in a batch\n",
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    try:\n",
    "        # it's a classification task, take the argmax\n",
    "        predictions_processed = np.argmax(predictions, axis=1)\n",
    "\n",
    "        # Calculate Pearson correlation\n",
    "        pearson, _ = pearsonr(predictions_processed, labels)\n",
    "\n",
    "        return {'pearson': pearson}\n",
    "    except Exception as e:\n",
    "        print(f\"Error in compute_metrics: {e}\")\n",
    "        return {'pearson': None}\n",
    "    \n",
    "#We will have a custom loss function that deals with the class weights and have class weights as additional argument in constructor\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Ensure label_weights is a tensor\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, num_items_in_batch=None, return_outputs=False):\n",
    "        # Extract labels and convert them to long type for cross_entropy\n",
    "        labels = inputs.pop(\"labels\").long()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Extract logits assuming they are directly outputted by the model\n",
    "        logits = outputs.get('logits')\n",
    "\n",
    "        # Compute custom loss with class weights for imbalanced data handling\n",
    "        if self.class_weights is not None:\n",
    "            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "training_args = TrainingArguments(\n",
    "    output_dir = 'sequence_classification',\n",
    "    learning_rate = 1e-3,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    num_train_epochs = 5,\n",
    "    weight_decay = 0.01,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end = True\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_datasets['train'],\n",
    "    eval_dataset = tokenized_datasets['val'],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = collate_fn,\n",
    "    compute_metrics = compute_metrics,\n",
    "    class_weights=class_weights,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "def make_predictions(model, df):\n",
    "    \n",
    "\n",
    "    sentences = df.input.tolist()\n",
    "\n",
    "      # Define the batch size\n",
    "    batch_size = 32  # You can adjust this based on your system's memory capacity\n",
    "\n",
    "      # Initialize an empty list to store the model outputs\n",
    "    all_outputs = []\n",
    "\n",
    "      # Process the sentences in batches\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "          # Get the batch of sentences\n",
    "        batch_sentences = sentences[i:i + batch_size]\n",
    "\n",
    "          # Tokenize the batch\n",
    "        inputs = tokenizer(batch_sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "          # Move tensors to the device where the model is (e.g., GPU or CPU)\n",
    "        inputs = {k: v.to('cuda' if torch.cuda.is_available() else 'cpu') for k, v in inputs.items()}\n",
    "\n",
    "          # Perform inference and store the logits\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            all_outputs.append(outputs['logits'])\n",
    "\n",
    "    final_outputs = torch.cat(all_outputs, dim=0)\n",
    "    df['predictions']=final_outputs.argmax(axis=1).cpu().numpy()\n",
    "    #df['predictions']=df['predictions'].apply(lambda l:category_map[l])\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_performance_metrics(df):\n",
    "    y_test = df.Institutional_Form_category\n",
    "    y_pred = df.predictions\n",
    "    print(f\"comparing test {y_test} and pred {y_pred}\")\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(\"Balanced Accuracy Score:\", balanced_accuracy_score(y_test, y_pred))\n",
    "    print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "df_val = make_predictions(model, tokenizer, df_val)\n",
    "\n",
    "#make_predictions(model,df_val)\n",
    "\n",
    "get_performance_metrics(df_val)\n",
    "#df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e93f512-eaa0-4350-8431-3ddf6a66cc42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparing test 56     2\n",
      "158    3\n",
      "261    0\n",
      "238    0\n",
      "329    2\n",
      "      ..\n",
      "72     3\n",
      "2      1\n",
      "408    3\n",
      "318    3\n",
      "509    3\n",
      "Name: Institutional_Form_category, Length: 93, dtype: int8 and pred 56     0\n",
      "158    3\n",
      "261    0\n",
      "238    2\n",
      "329    3\n",
      "      ..\n",
      "72     3\n",
      "2      1\n",
      "408    3\n",
      "318    3\n",
      "509    3\n",
      "Name: predictions, Length: 93, dtype: int64\n",
      "Confusion Matrix:\n",
      "[[ 4  3  6  5]\n",
      " [ 1 10  3  1]\n",
      " [ 2  1  2  4]\n",
      " [ 0  2  5 44]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.22      0.32        18\n",
      "           1       0.62      0.67      0.65        15\n",
      "           2       0.12      0.22      0.16         9\n",
      "           3       0.81      0.86      0.84        51\n",
      "\n",
      "    accuracy                           0.65        93\n",
      "   macro avg       0.53      0.49      0.49        93\n",
      "weighted avg       0.67      0.65      0.64        93\n",
      "\n",
      "Balanced Accuracy Score: 0.4934640522875817\n",
      "Accuracy Score: 0.6451612903225806\n"
     ]
    }
   ],
   "source": [
    "df_val = make_predictions(model, df_val)\n",
    "\n",
    "#make_predictions(model,df_val)\n",
    "\n",
    "get_performance_metrics(df_val)\n",
    "#df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83590ad5-5b1e-42a0-8080-046cbf4d9c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llama]",
   "language": "python",
   "name": "conda-env-llama-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
