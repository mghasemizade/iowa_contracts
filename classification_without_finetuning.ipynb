{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b397b44a-3255-4a87-9d07-ebbd138154fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498\n",
      "62\n",
      "63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 10/63 [00:16<01:16,  1.44s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|██████████| 63/63 [01:31<00:00,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.063\n",
      "Accuracy for label joint operations: 0.200\n",
      "Accuracy for label new joint entities: 0.000\n",
      "Accuracy for label resource sharing: 0.000\n",
      "Accuracy for label service contract: 0.026\n",
      "\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "  joint operations       0.20      0.20      0.20        15\n",
      "new joint entities       0.00      0.00      0.00         6\n",
      "  resource sharing       0.00      0.00      0.00         4\n",
      "  service contract       1.00      0.03      0.05        38\n",
      "\n",
      "         micro avg       0.25      0.06      0.10        63\n",
      "         macro avg       0.30      0.06      0.06        63\n",
      "      weighted avg       0.65      0.06      0.08        63\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3 0 0 0]\n",
      " [2 0 0 0]\n",
      " [1 0 0 0]\n",
      " [9 0 0 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/u/mghasemizade/miniconda3/envs/llama/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "from transformers import (AutoModelForCausalLM, \n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig, \n",
    "                          TrainingArguments, \n",
    "                          pipeline, \n",
    "                          logging)\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftConfig\n",
    "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on {device}\")\n",
    "\n",
    "base_model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"float16\", \n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "df = pd.read_csv('filtered_labeled.csv')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove newline-separated letters\n",
    "    cleaned_text = re.sub(r'(\\n[a-zA-Z])', '', text)\n",
    "    \n",
    "    # Remove single newlines or extraneous spaces\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    cleaned_text = re.sub(r'11.', ' ', cleaned_text).strip()\n",
    "    cleaned_text = re.sub(r\"\\n\\'\", ' ', cleaned_text).strip()\n",
    "    cleaned_text = re.sub(r'~', ' ', cleaned_text).strip()\n",
    "    cleaned_text = re.sub(r'\\n1', ' ', cleaned_text).strip()\n",
    "    cleaned_text = re.sub(r'~{2}', ' ', cleaned_text).strip()\n",
    "    cleaned_text = re.sub(r'\\({2}', ' ', cleaned_text).strip()\n",
    "    cleaned_text = re.sub(r'\\){2}', ' ', cleaned_text).strip()\n",
    "    return cleaned_text\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "df = df.sample(frac=1, random_state=85).reset_index(drop=True)\n",
    "\n",
    "system_prompt = \"You are a helpful annotator for a dataset of legal contracts between entities in Iowa state.\"\n",
    "pre_condition = \"The class name is \"\n",
    "\n",
    "train_end = int(0.8 * len(df))\n",
    "eval_end = train_end + int(0.1 * len(df))\n",
    "\n",
    "# Split the data\n",
    "X_train = df[:train_end]\n",
    "print(len(X_train))\n",
    "X_eval = df[train_end:eval_end]\n",
    "print(len(X_eval))\n",
    "X_test = df[eval_end:]\n",
    "print(len(X_test))\n",
    "\n",
    "# Categories and prompts\n",
    "class_names = [\n",
    "    'joint operations', 'new joint entities', 'resource sharing', 'service contract']\n",
    "\n",
    "# Define the prompt generation functions\n",
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "Given this contract :\n",
    "{data_point[\"text\"]}\n",
    "\n",
    "\"You are a contract classification assistant. Your task is to classify the contract text \"\n",
    "        \"into one of the predefined categories. Here are the criteria for each category:\\n\"\n",
    "        \"- Joint Operations: Partnership arrangements to jointly produce services with one or more organizations.\\n\"\n",
    "        \"- New Joint Entities: Two or more organizations creating a separate new entity to manage or govern a shared asset or service.\\n\"\n",
    "        \"- Resource Sharing: Sharing of information, personnel, equipment, etc., between governments or community organizations to provide services.\\n\"\n",
    "        \"- Service Contracts: Agreements with outside entities, public or private, for provision or support services.\\n\"\n",
    "        \"Analyze the given text carefully and respond with the appropriate category.\"\n",
    "\n",
    "\"\"\"\n",
    "#            Classify the text into {class_names} and return the answer as the govermental contract label. Give me your best guess.\n",
    "#text: {data_point[\"text\"]}\n",
    "#label: {data_point[\"Institutional_Form\"]}\"\"\".strip()\n",
    "\n",
    "def generate_test_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "Given this contract :\n",
    "{data_point[\"text\"]}\n",
    "\n",
    "\"You are a contract classification assistant. Your task is to classify the contract text \"\n",
    "        \"into one of the predefined categories. Here are the criteria for each category:\\n\"\n",
    "        \"- Joint Operations: Partnership arrangements to jointly produce services with one or more organizations.\\n\"\n",
    "        \"- New Joint Entities: Two or more organizations creating a separate new entity to manage or govern a shared asset or service.\\n\"\n",
    "        \"- Resource Sharing: Sharing of information, personnel, equipment, etc., between governments or community organizations to provide services.\\n\"\n",
    "        \"- Service Contracts: Agreements with outside entities, public or private, for provision or support services.\\n\"\n",
    "        \"Analyze the given text carefully and respond with the appropriate category.\"\n",
    "\n",
    "\"\"\"\n",
    "#            Classify the text into {class_names} and return the answer as the govermental contract label. Give me your best guess.\n",
    "#text: {data_point[\"text\"]}\n",
    "#label: \"\"\".strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Generate prompts for training and evaluation data\n",
    "X_train.loc[:,'text'] = X_train.apply(generate_prompt, axis=1)\n",
    "X_eval.loc[:,'text'] = X_eval.apply(generate_prompt, axis=1)\n",
    "\n",
    "# Generate test prompts and extract true labels\n",
    "y_true = X_test.loc[:,'Institutional_Form']\n",
    "X_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"text\"])\n",
    "\n",
    "\n",
    "\n",
    "# Convert to datasets\n",
    "train_data = Dataset.from_pandas(X_train[[\"text\"]])\n",
    "eval_data = Dataset.from_pandas(X_eval[[\"text\"]])\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", \n",
    "                        model=model, \n",
    "                        tokenizer=tokenizer, \n",
    "                        max_new_tokens=50, \n",
    "                        temperature=0.1)\n",
    "\n",
    "def predict(test, model, tokenizer):\n",
    "    y_pred = []\n",
    "    categories = ['joint operations', 'new joint entities', 'resource sharing', 'service contract']\n",
    "    \n",
    "    for i in tqdm(range(len(test))):\n",
    "        prompt = test.iloc[i][\"text\"]\n",
    "        #print(prompt)\n",
    "        \n",
    "        \n",
    "        \n",
    "        result = pipe(prompt)\n",
    "        #print(result)\n",
    "        input_length = len(prompt)\n",
    "        #answer = result#[input_length:]\n",
    "        answer = result[0]['generated_text'][len(prompt):].strip() #.split(\"label:\")[-1].strip()\n",
    "        #print(answer)\n",
    "        \n",
    "        # Determine the predicted category\n",
    "        for category in categories:\n",
    "            if category.lower() in str(answer).lower():\n",
    "                y_pred.append(category)\n",
    "                break\n",
    "        else:\n",
    "            y_pred.append(\"none\")\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "y_pred = predict(X_test, model, tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    labels = ['joint operations', 'new joint entities', 'resource sharing', 'service contract']\n",
    "\n",
    "    mapping = {label: idx for idx, label in enumerate(labels)}\n",
    "    \n",
    "    def map_func(x):\n",
    "        return mapping.get(x, -1)  # Map to -1 if not found, but should not occur with correct data\n",
    "    \n",
    "    y_true_mapped = np.vectorize(map_func)(y_true)\n",
    "    y_pred_mapped = np.vectorize(map_func)(y_pred)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true=y_true_mapped, y_pred=y_pred_mapped)\n",
    "    print(f'Accuracy: {accuracy:.3f}')\n",
    "    \n",
    "    # Generate accuracy report\n",
    "    unique_labels = set(y_true_mapped)  # Get unique labels\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        label_indices = [i for i in range(len(y_true_mapped)) if y_true_mapped[i] == label]\n",
    "        label_y_true = [y_true_mapped[i] for i in label_indices]\n",
    "        label_y_pred = [y_pred_mapped[i] for i in label_indices]\n",
    "        label_accuracy = accuracy_score(label_y_true, label_y_pred)\n",
    "        print(f'Accuracy for label {labels[label]}: {label_accuracy:.3f}')\n",
    "        \n",
    "    # Generate classification report\n",
    "    class_report = classification_report(y_true=y_true_mapped, y_pred=y_pred_mapped, target_names=labels, labels=list(range(len(labels))))\n",
    "    print('\\nClassification Report:')\n",
    "    print(class_report)\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true=y_true_mapped, y_pred=y_pred_mapped, labels=list(range(len(labels))))\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "\n",
    "evaluate(y_true, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9c7ab93-4a9c-4929-a56e-42cfa6fc1989",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "\n",
    "# Clear GPU memory (if applicable)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e09110d9-8664-4701-874c-345b5227fce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.26s/it]\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.1-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 498/498 [00:00<00:00, 1606.78 examples/s]\n",
      "Map: 100%|██████████| 125/125 [00:00<00:00, 1601.25 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparing test 249    0\n",
      "558    3\n",
      "174    1\n",
      "280    0\n",
      "110    1\n",
      "      ..\n",
      "6      3\n",
      "104    3\n",
      "114    3\n",
      "355    3\n",
      "132    3\n",
      "Name: Institutional_Form_category, Length: 125, dtype: int8 and pred 249    1\n",
      "558    3\n",
      "174    0\n",
      "280    1\n",
      "110    1\n",
      "      ..\n",
      "6      1\n",
      "104    3\n",
      "114    1\n",
      "355    1\n",
      "132    1\n",
      "Name: predictions, Length: 125, dtype: int64\n",
      "Confusion Matrix:\n",
      "[[ 2  9  5  5]\n",
      " [ 1 13  5  2]\n",
      " [ 0  5  2  3]\n",
      " [ 5 38  6 24]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.10      0.14        21\n",
      "           1       0.20      0.62      0.30        21\n",
      "           2       0.11      0.20      0.14        10\n",
      "           3       0.71      0.33      0.45        73\n",
      "\n",
      "    accuracy                           0.33       125\n",
      "   macro avg       0.32      0.31      0.26       125\n",
      "weighted avg       0.50      0.33      0.35       125\n",
      "\n",
      "Balanced Accuracy Score: 0.3107632093933464\n",
      "Accuracy Score: 0.328\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import functools\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import evaluate\n",
    "import re\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, balanced_accuracy_score, accuracy_score\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from datasets import Dataset, DatasetDict\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "\n",
    "# Load and preprocess dataset\n",
    "df = pd.read_csv('filtered_labeled.csv')\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'(\\n[a-zA-Z])', '', text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    return cleaned_text\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "df['Institutional_Form'] = df['Institutional_Form'].astype('category')\n",
    "df['Institutional_Form_category'] = df['Institutional_Form'].cat.codes\n",
    "\n",
    "category_map = {code: category for code, category in enumerate(df['Institutional_Form'].cat.categories)}\n",
    "\n",
    "df_train, df_val = train_test_split(df, train_size=0.8, test_size=0.2, random_state=42)\n",
    "\n",
    "def generate_features_with_prompt(df):\n",
    "    instruction = (\n",
    "        \"You are a contract classification assistant. Your task is to classify the contract text \"\n",
    "        \"into one of the predefined categories. Here are the criteria for each category:\\n\"\n",
    "        \"- Joint Operations: Partnership arrangements to jointly produce services with one or more organizations.\\n\"\n",
    "        \"- New Joint Entities: Two or more organizations creating a separate new entity to manage or govern a shared asset or service.\\n\"\n",
    "        \"- Resource Sharing: Sharing of information, personnel, equipment, etc., between governments or community organizations to provide services.\\n\"\n",
    "        \"- Service Contracts: Agreements with outside entities, public or private, for provision or support services.\\n\"\n",
    "        \"Analyze the given text carefully and respond with the appropriate category.\"\n",
    "    )\n",
    "    df['Institutional_Form'] = df['Institutional_Form'].astype(str)\n",
    "    df['input'] = (\n",
    "        instruction\n",
    "        + \"\\n\\nContract Text: \"\n",
    "        + df['text']\n",
    "        + \"\\n\\nInstitutional Form Category: \"\n",
    "        + df['Institutional_Form']\n",
    "    )\n",
    "    return df\n",
    "\n",
    "generate_features_with_prompt(df_train)\n",
    "generate_features_with_prompt(df_val)\n",
    "\n",
    "dataset_train = Dataset.from_pandas(df_train.reset_index(drop=True))\n",
    "dataset_val = Dataset.from_pandas(df_val.reset_index(drop=True))\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': dataset_train,\n",
    "    'val': dataset_val,\n",
    "})\n",
    "\n",
    "# Load pre-trained model\n",
    "model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, \n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    num_labels=len(category_map),\n",
    "    device_map=\"auto\"  # Automatically distribute model layers across GPUs\n",
    ")\n",
    "# Update the model configuration\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False  # Ensure compatibility with fine-tuning\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def llama_preprocessing_function(examples):\n",
    "    return tokenizer(examples['input'], truncation=True, max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(llama_preprocessing_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"Institutional_Form_category\", \"label\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Collate function\n",
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Prediction and evaluation\n",
    "def make_predictions(model, df):\n",
    "    # Ensure padding token is defined\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    sentences = df.input.tolist()\n",
    "\n",
    "    batch_size = 32  # Adjust based on system capacity\n",
    "    all_outputs = []\n",
    "\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch_sentences = sentences[i:i + batch_size]\n",
    "        inputs = tokenizer(batch_sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to('cuda' if torch.cuda.is_available() else 'cpu') for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            all_outputs.append(outputs['logits'])\n",
    "\n",
    "    final_outputs = torch.cat(all_outputs, dim=0)\n",
    "    df['predictions'] = final_outputs.argmax(axis=1).cpu().numpy()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_performance_metrics(df):\n",
    "    y_test = df.Institutional_Form_category\n",
    "    y_pred = df.predictions\n",
    "    print(f\"comparing test {y_test} and pred {y_pred}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Balanced Accuracy Score:\", balanced_accuracy_score(y_test, y_pred))\n",
    "    print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "df_val = make_predictions(model, df_val)\n",
    "get_performance_metrics(df_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b723b765-984c-45a3-b0e7-55ad11bbdbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('filtered_labeled.csv')\n",
    "\n",
    "\n",
    "df['Institutional_Form'] = df['Institutional_Form'].astype('category')\n",
    "df['Institutional_Form_category'] = df['Institutional_Form'].cat.codes\n",
    "category_map = {code: category for code, category in enumerate(df['Institutional_Form'].cat.categories)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08ee9cf9-a465-4787-b298-a25f98bbf657",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'joint operations',\n",
       " 1: 'new joint entities',\n",
       " 2: 'resource sharing',\n",
       " 3: 'service contract'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db285447-3789-4d82-9492-b7671c072f37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llama]",
   "language": "python",
   "name": "conda-env-llama-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
